{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de2feb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from google.oauth2 import service_account\n",
    "from selenium.webdriver.common.by import By\n",
    "from googleapiclient.errors import HttpError\n",
    "import csv\n",
    "import PyPDF2\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "import requests\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver import ChromeOptions\n",
    "web_pages = dict()\n",
    "import vlc\n",
    "import pdfplumber\n",
    "import regex as re\n",
    "import pdb\n",
    "import regex as re\n",
    "# from PyPDF2 import PdfFileReader, PdfFileWriter\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import os.path\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "\n",
    "SAMPLE_SPREADSHEET_ID = '1kge8VOTe7oUNFagXmP2f3HdgI0j4-y2wOQlcTjuuTy0'\n",
    "\n",
    "SAMPLE_RANGE_NAME = 'A2:D7'\n",
    "p = vlc.MediaPlayer(\"/Users/oluwaseuncardoso/PythonStuff/AlarmNuclearMeltd HYP013301_preview.mp3\")\n",
    "\n",
    "\n",
    "def findLink(tag):\n",
    "    if tag.name == \"span\" and tag.a != None and tag.get(\"class\") != None:\n",
    "        cond = \"gateway.proquest.com\"\n",
    "        if tag[\"class\"][0] == 'subjectField-postProcessingHook' and cond in tag.a[\"href\"]:\n",
    "            return True\n",
    "    return False\n",
    "def findCapcha(tag):\n",
    "    if tag.name ==\"div\" and div.get(\"id\") == \"start\":\n",
    "        if tag.div.get(\"class\") == \"alert alert-info container captcha_alert_box\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def getBs(link):\n",
    "    \"\"\"\n",
    "    This function takes a proquest Url\n",
    "    and returns the beautiful soup object\n",
    "    \"\"\"\n",
    "    driver.get(link)\n",
    "    sleep(3)\n",
    "    html = driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "    bs = BeautifulSoup(html, \"html5lib\")\n",
    "    capcha1 = bs.find_all(\"div\", attrs = {\"class\": \"alert alert-info container captcha_alert_box\"} )\n",
    "    playing = False\n",
    "    while(len(capcha1) != 0 ):\n",
    "        print(\"start playing alarm\")\n",
    "        p.play()\n",
    "        playing = True\n",
    "        sleep(5)\n",
    "        break\n",
    "    if(playing == True):\n",
    "        p.stop()\n",
    "    return bs\n",
    "\n",
    "def goToPage(num): \n",
    "    \"\"\"\n",
    "    This function takes the browser to the specific page num\n",
    "    This fucntion is useful because sometimes mid way in downloading.\n",
    "    A bot check stops the program.\n",
    "    We need a way to go back to our intended page number to continue downloading\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def writeContent(pdf): # come here to change code\n",
    "    response = requests.get(pdf) \n",
    "    with open('./dissertation.pdf', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "        sleep(4)  \n",
    "    ack1 = findAck()\n",
    "    ack2 = findAck2()\n",
    "    return ack1, ack2\n",
    "\n",
    "def getAckFromLines(lines,ACK2=None):\n",
    "    # numeral numbers\n",
    "    regex1 = r\"[^\\w]+i$+|^i$|^1[^\\w]+|[^\\w]+1$+|^1$|\"\n",
    "    regex2 = r\"^ii[^\\w]+|[^\\w]+ii$+|^ii$|^2[^\\w]+|[^\\w]+2$+|^2$|\"\n",
    "    regex3 = r\"^iii[^\\w]+|[^\\w]+iii$+|^iii$|^3[^\\w]+|[^\\w]+3$+|^3$|\"\n",
    "    regex4 = r\"^iv[^\\w]+|[^\\w]+iv$+|^iv$|^4[^\\w]+|[^\\w]+4$+|^4$|\"\n",
    "    regex5 = r\"^v[^\\w]+|[^\\w]+v$+|^v$|^5[^\\w]+|[^\\w]+5$+|^5$|\"\n",
    "    regex6 = r\"^vi[^\\w]+|[^\\w]+vi$+|^vi$|^6[^\\w]+|[^\\w]+6$+|^6$|\"\n",
    "    regex7 = r\"^vii[^\\w]+|[^\\w]+vii$+|^vii$|^7[^\\w]+|[^\\w]+7$+|^7$|\"\n",
    "    regex8 = r\"^viii[^\\w]+|[^\\w]+viii$+|^viii$|^8[^\\w]+|[^\\w]+8$+|^8$|\"\n",
    "    regex9 = r\"^ix[^\\w]+|[^\\w]+ix$+|^ix$|^9[^\\w]+|[^\\w]+9$+|^9$|\"\n",
    "    regex10 = r\"^x[^\\w]+|[^\\w]+x$+|^x$|^10[^\\w]+|[^\\w]+10$+|^10$\"\n",
    "    regex = regex1+regex2+regex3+regex4+regex5+regex6+regex7+regex8+regex9+regex10\n",
    "    \n",
    "    ack_index = -1\n",
    "    if ACK2 != None:\n",
    "        for i in range(len(lines)):\n",
    "            if re.search(ACK2, lines[i],  re.IGNORECASE):\n",
    "                ack_index = i\n",
    "                break\n",
    "        lines = lines[ack_index+1:]\n",
    "\n",
    "    # remove every after page number(if page number exists)\n",
    "    end_line = len(lines) \n",
    "    half_page_line = len(lines)//2 # the page number will must likes be between the bottom and half the page\n",
    "    last_page_line = len(lines)-1\n",
    "    for i in range(last_page_line, half_page_line, -1):\n",
    "        if re.search(regex, lines[i]):\n",
    "            #pdb.set_trace()\n",
    "            end_line = i\n",
    "            break\n",
    "    return lines[:end_line]\n",
    "    \n",
    "def findAck(pdf_path = \"./dissertation.pdf\",from_page = 0,to_page = 29):\n",
    "    \"\"\"\n",
    "    create a new pdf file from a subsection from pdf\n",
    "    from_page(int): Where to start. Starts from 0.\n",
    "    to_page(int): Where to end(inclusive).\n",
    "    \"\"\"\n",
    "    pdf = PdfReader(pdf_path)\n",
    "    num_pages = len(pdf.pages)\n",
    "    if to_page > num_pages:\n",
    "        to_page = num_pages-1\n",
    "    ack_found = None\n",
    "    ack_extracted, content_extracted = False, False\n",
    "    acknowledgment = \"Acknoledgement not present in file\"\n",
    "    for page_num in range(from_page, to_page):\n",
    "        try: \n",
    "            pdfWriter = PdfWriter()\n",
    "            pdfWriter.add_page(pdf.pages[page_num])   \n",
    "            with open(f'./temp.pdf', 'wb') as temp:\n",
    "                pdfWriter.write(temp)\n",
    "            temp.close()\n",
    "            content = convertToString().strip()\n",
    "        except PyPDF2.errors.PdfReadError:\n",
    "            return {\"bool\" : False, \"content\" :\"PDF Broken\"}\n",
    "        if(ack_found == None):\n",
    "            ACK = r'ack[n]?owledg[e]?ment[s]?\\s*\\n'\n",
    "            ack_found = re.search(ACK,content, re.IGNORECASE)\n",
    "        if(ack_found != None and ack_extracted == False):\n",
    "            #pdb.set_trace()\n",
    "            ACK2 = ack_found[0].split(\"\\n\")[0] \n",
    "            #find the index to find the index in lines\n",
    "            \n",
    "            try:\n",
    "                ack_index = content.split(\"\\n\").index(ACK2)\n",
    "            except ValueError:\n",
    "                return {\"bool\" : False , \"content\" : \"Error: Special case please review\"}\n",
    "            \n",
    "            if ack_index > 4:\n",
    "                ack_found = None\n",
    "            else:\n",
    "                acknowledgment =  content.strip()     \n",
    "                lines = acknowledgment.split('\\n')          \n",
    "                lines = getAckFromLines(lines,ACK2)\n",
    "                acknowledgment = \"\\n\".join(lines)\n",
    "                ack_extracted  = True\n",
    "            continue # start loop again\n",
    "        if(ack_extracted == True and content_extracted == False):\n",
    "            #pdb.set_trace()\n",
    "            lines = content.split('\\n')\n",
    "            next_title = lines[ack_index]\n",
    "            words_of_next_title = next_title.split(\" \")\n",
    "            words_of_next_title = [i for i in words_of_next_title if i not in [\"\",\" \"]]\n",
    "            #pdb.set_trace()\n",
    "            there_is_no_title = len(words_of_next_title) > 6\n",
    "            if (there_is_no_title):   \n",
    "                lines = content.strip().split('\\n')          \n",
    "                lines = getAckFromLines(lines)\n",
    "                acknowledgment += \"\\n\".join(lines)\n",
    "                last_word = lines[-1]                    \n",
    "            else:\n",
    "                content_extracted = True\n",
    "        if content_extracted == True:            \n",
    "            return {\"bool\" : True, \"content\" :acknowledgment}\n",
    "        \n",
    "       \n",
    "    return {\"bool\" : False, \"content\" :\"ACKNOTFOUND\"}\n",
    "\n",
    "def convertToString():\n",
    "    images = convert_from_path(\"./temp.pdf\")\n",
    "    content = \"\"\n",
    "    images[0].save(f\"./temp.jpg\",\"JPEG\")\n",
    "    content = pytesseract.image_to_string(f\"./temp.jpg\")\n",
    "    return content\n",
    "import numpy as np\n",
    "def Levenshtein(r, h):\n",
    "    \"\"\"                                                                         \n",
    "    Calculation of WER with Levenshtein distance.                               \n",
    "                                                                                \n",
    "    Works only for iterables up to 254 elements (uint8).                        \n",
    "    O(nm) time ans space complexity.                                            \n",
    "                                                                                \n",
    "    Parameters                                                                  \n",
    "    ----------                                                                  \n",
    "    r : list of strings                                                                    \n",
    "    h : list of strings                                                                   \n",
    "                                                                                \n",
    "    Returns                                                                     \n",
    "    -------                                                                     \n",
    "    (WER, nS, nI, nD): (float, int, int, int) WER, number of substitutions, insertions, and deletions respectively\n",
    "                                                                                \n",
    "    Examples                                                                    \n",
    "    --------                                                                    \n",
    "    >>> wer(\"who is there\".split(), \"is there\".split())                         \n",
    "    0.333 0 0 1                                                                           \n",
    "    >>> wer(\"who is there\".split(), \"\".split())                                 \n",
    "    1.0 0 0 3                                                                           \n",
    "    >>> wer(\"\".split(), \"who is there\".split())      #ask in Pia\n",
    "    Inf 0 3 0                                                                           \n",
    "    \"\"\"\n",
    "\n",
    "    n = len(r) # The number of words in REF\n",
    "    m = len(h) # The number of words in HYP\n",
    "    R = np.zeros((n+1,m+1))\n",
    "    B = np.zeros((n+1,m+1))\n",
    "\n",
    "    #for all i,j s.t.  i = 0 or  j = 0,\tset\tR[i,j] ‚Üê max (i,j) end\n",
    "    R[0,:] = np.arange(m+1)\n",
    "    R[:,0] = np.arange(n+1)\n",
    "    # i think we should do this aswell\n",
    "    up = 0\n",
    "    left = 1\n",
    "    up_left = 2\n",
    "    up_left2 = 3\n",
    "    B[0,:] = left\n",
    "    B[:,0] = up\n",
    "    B[0,0] = up\n",
    "    for i in range(1,n+1):\n",
    "        for j in range(1,m+1):\n",
    "            dele = R[i - 1, j] + 1 # delete\n",
    "            sub = R[i - 1, j - 1] + (1,0)[r[i-1] == h[j-1]] #substitute #NOTE look at this\n",
    "            ins = R[i, j-1] + 1 #insert\n",
    "\n",
    "            R[i,j] = min(dele,sub,ins)\n",
    "            if R[i,j] == dele:\n",
    "                B[i , j] = up\n",
    "            elif R[i , j] == ins:\n",
    "                B[i,j] = left\n",
    "            else:\n",
    "                B[i,j] = (up_left, up_left2 )[r[i-1] == h[j-1]]\n",
    "    i,j = n,m\n",
    "    nSub,nDel,nIns = 0, 0, 0\n",
    "    transversal = True\n",
    "    while transversal == True:\n",
    "        path = B[i,j]\n",
    "        if i <=  0 and j <=0:\n",
    "            transversal = False\n",
    "            break\n",
    "        if path == up_left:\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "            nSub += 1\n",
    "        elif path == left:\n",
    "            j -= 1\n",
    "            nIns +=1\n",
    "        elif path == up:\n",
    "            i -= 1\n",
    "            nDel +=1\n",
    "        else: # correct\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "    return R[n,m]/n, nSub, nIns, nDel        \n",
    "    \n",
    "def getAcknoledgement(page,pages):\n",
    "    \"\"\"\n",
    "    Rule:\n",
    "    1. Look for the first occurcance of Acknowledgments and a new line.\n",
    "    2. Caputure the content until you hit the title of the next section  \n",
    "    #ASSUMPTIONS that the dissertations need to follow.\n",
    "    #1. Every dissertation starts it's acknoledgement with the word \"Acknoledgement\" as the only word in it's title\n",
    "    #2. No two titles can occupy the same page. \n",
    "    #3. The acknoledgement starts with a capital letter (i.e Acknoledgement)\n",
    "    #4. The title acknoledgemen may or may not end an s.\n",
    "    #5. If a line has 1 to 6 words in it. Then it's a title of a new section.\n",
    "    #6. Titles are located on the same location on each page.i.e top and centre\n",
    "    #7. Each word is of average size(6 chars long)\n",
    "    #8. Target titles can either be Acknowledgments or acknowledg(e)ments\n",
    "    #9. The Acknoledgemts page doesn't have picture/images\n",
    "    #10. Acknoledgements are never in the first page\n",
    "    Params: \n",
    "    page : pdfplumber.page.Page\n",
    "    returns(String) :The content of the abstraction\n",
    "    \"\"\"    \n",
    "    initial_index = page.page_number -1\n",
    "    content = page.extract_text()\n",
    "    if(content is None): return None\n",
    "    content = content.strip()\n",
    "    ACK = r'ack[n]?owledg[e]?ment[s]?\\s*\\n'\n",
    "    ack_found = re.search(ACK,content, re.IGNORECASE)\n",
    "    title_len = len(ACK)\n",
    "    acknowledgment = None\n",
    "    #if Acknowledgement is the title extract it else continue\n",
    "    if(ack_found):\n",
    "        #pdb.set_trace()\n",
    "        ACK2 = ack_found[0].split(\"\\n\")[0] \n",
    "        acknowledgment =  content.strip()     \n",
    "        lines = acknowledgment.split('\\n')          \n",
    "        lines = getAckFromLines(lines,ACK2)\n",
    "        acknowledgment = \"\\n\".join(lines)        \n",
    "        stop_not_true = True\n",
    "        #find the index to find the index in lines\n",
    "        try:\n",
    "            ack_line = content.strip().split(\"\\n\")\n",
    "            ack_index = ack_line.index(ACK2)\n",
    "            ack_line = [ack_line[i] for i in range(ack_index+1) if i not in [\"\", \" \"]]\n",
    "            ack_index = ack_line.index(ACK2)\n",
    "        except ValueError:\n",
    "            return \"Error: Special case please revuew\"\n",
    "        counter = 1\n",
    "        while(stop_not_true):\n",
    "            next_page = counter + initial_index\n",
    "            #pdb.set_trace()\n",
    "            page = pages[next_page]\n",
    "            content = page.extract_text().lower().lstrip()\n",
    "            lines = content.split('\\n')\n",
    "            next_title = lines[ack_index]\n",
    "            words_of_next_title = next_title.split(\" \")\n",
    "            words_of_next_title = [i for i in words_of_next_title if i not in [\"\",\" \"]]\n",
    "            #pdb.set_trace()\n",
    "            there_is_no_title = len(words_of_next_title) > 6\n",
    "            if (there_is_no_title):   \n",
    "                lines = content.strip().split('\\n')          \n",
    "                lines = getAckFromLines(lines)\n",
    "                acknowledgment += \"\\n\".join(lines)\n",
    "                last_word = lines[-1]                    \n",
    "                counter+=1\n",
    "            else:\n",
    "                stop_not_true = False\n",
    "    return acknowledgment\n",
    "\n",
    "def findAck2(pdf_path = \"./dissertation.pdf\",from_page = 0,to_page = 29):\n",
    "    with pdfplumber.open(pdf_path, strict_metadata=True) as pdf:\n",
    "        pages = iter(pdf.pages)\n",
    "        count = 0\n",
    "        \n",
    "        for page in pages:\n",
    "            count +=1\n",
    "            if count >= from_page: \n",
    "                content = page.extract_text()        \n",
    "                acknowledgement = getAcknoledgement(page, pdf.pages)    \n",
    "                if(acknowledgement != None):\n",
    "                    if(\"(cid:\" in acknowledgement):\n",
    "                        acknowledgement = \"ERROR: It contains embedded fonts. This requires revisitation.\"\n",
    "                        \n",
    "                    return {\"bool\": True ,  \"content\": acknowledgement}\n",
    "            if page.page_number == to_page:\n",
    "                print(\"ack not found\")\n",
    "                return {\"bool\": False, \"content\":\"ACKNOTFOUND\"  }\n",
    "            \n",
    "\n",
    "def compare(ack1,ack2):\n",
    "    if(ack1['bool'] == True and ack2['bool']  == True ):\n",
    "        content1 = ack1[\"content\"].replace(\"\\n\", \"\").strip().split()\n",
    "        content2 = ack2[\"content\"].replace(\"\\n\", \"\").strip().split()\n",
    "        WER = Levenshtein(content1, content2)[0]\n",
    "        print(WER)\n",
    "        if(WER < 0.2):\n",
    "            return ack1[\"content\"], \"N/A: same as 1st parser\"          \n",
    "    return ack1[\"content\"] , ack2[\"content\"]\n",
    "\n",
    "\n",
    "def main_alt():\n",
    "    \"\"\"Shows basic usage of the Sheets API.\n",
    "    Prints values from a sample spreadsheet.\n",
    "    \"\"\"\n",
    "    # If modifying these scopes, delete the file token.json.\n",
    "    SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "    creds = None\n",
    "    # The file token.json stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())            \n",
    "    return creds\n",
    "\n",
    "def main():\n",
    "    SCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/cloud-platform']\n",
    "    SERVICE_ACCOUNT_FILE = '../Keys/simpeapis-ff416f56b1c4.json' \n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "            SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    return credentials\n",
    "    \n",
    "creds = main()\n",
    "service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "def send(data, cells, service):\n",
    "    \n",
    "    value_range_body = {\n",
    "        \"majorDimension\" : \"ROWS\",\n",
    "        \"values\" : \n",
    "            [\n",
    "            [data['author'],data['title'], data['ack1'],data['ack2']],\n",
    "            ]\n",
    "    }\n",
    "\n",
    "    # have a program that send s batch as oppossed to just one\n",
    "    sheet = service.spreadsheets()\n",
    "    request = sheet.values().update(spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "                          range=cells,\n",
    "                          valueInputOption ='RAW',\n",
    "                          includeValuesInResponse = True,\n",
    "                          body = value_range_body )\n",
    "    try:\n",
    "        return request.execute()\n",
    "    except ConnectionResetError:\n",
    "        print(\"ConnectionResetError\")\n",
    "        creds = main()\n",
    "        service = build('sheets', 'v4', credentials=creds)\n",
    "        return send(data, cells, service)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7df68526",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def start_driver():\n",
    "    userdatadir = '/Users/profq/Desktop/Projects/PythonProjects/Research-paper-parser/user_data'\n",
    "    chromeOptions = ChromeOptions() \n",
    "    chromeOptions.add_argument(f\"--user-data-dir={userdatadir}\")\n",
    "    chromeOptions.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chromeOptions.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    s = Service(\"/Users/oluwaseuncardoso/Downloads/chromedriver\")\n",
    "\n",
    "    driver = webdriver.Chrome(service = s, options = chromeOptions)\n",
    "    driver.set_page_load_timeout(60)\n",
    "    #12-i#VFZ4eVWZ8fq6V\n",
    "    driver.get(\"https://www.proquest.com/\")\n",
    "    return driver\n",
    "driver = start_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a355121",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium.common.exceptions import WebDriverException\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b4cb9a6",
   "metadata": {},
   "source": [
    "# Go to specific page\n",
    "Play the tab below and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9ef7a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_to_page(page, items_per_page, start, driver=driver):\n",
    "    driver.get(\"https://www.proquest.com/\")\n",
    "    sleep(5)\n",
    "    try:\n",
    "        elem = driver.find_element(By.ID,\"queryTermField\")  \n",
    "    except  NoSuchElementException:\n",
    "        elem = driver.find_element(By.ID,\"searchTerm\")\n",
    "    elem.clear()\n",
    "    elem.send_keys(\"STYPE(DISSERTATION) AND PD(2010-2016) AND DEP.X(PSYCHOLOGY) AND LA(ENGLISH) AND DG(PHD) AND (ULO(UNITED STATES))  NOT SU(CLINICAL)\")\n",
    "    elem.send_keys(Keys.RETURN)\n",
    "    sleep(13)\n",
    "\n",
    "    dropdown = Select(driver.find_element(By.ID,\"itemsPerPage\"))\n",
    "    dropdown.select_by_value(str(items_per_page))\n",
    "    sleep(15)\n",
    "\n",
    "    search = driver.find_element(By.ID,\"pageNbrField\") \n",
    "    search.clear()\n",
    "    search.send_keys(str(page))\n",
    "    search.send_keys(Keys.RETURN)\n",
    "\n",
    "    sleep(15)\n",
    "\n",
    "    html = driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "    bs = BeautifulSoup(html, \"html5lib\")\n",
    "    results = bs.find_all(attrs= {\"class\": \"resultItem ltr\"})\n",
    "    h1 = bs.find_all( \"h1\")[0]\n",
    "    num_results = h1.contents[0]\n",
    "    num_results = num_results.strip()\n",
    "    num_results = num_results.replace(',', \"\")\n",
    "    num_results = re.search(r'[0-9]*', num_results)\n",
    "    num_results = int(num_results[0])\n",
    "    next_page = bs.find_all(\"a\", attrs = {\"title\" : \"Next Page\"} )\n",
    "    count = (page -1) * items_per_page + start\n",
    "    return {'count' : count, 'num_results' : num_results, \"results\" : results , \"next_page\" : next_page}\n",
    "    \n",
    "#print(\"What page do you want to go to: \")\n",
    "page = 11 #int(input())\n",
    "\n",
    "# How many pages per page do you want to see 10, 20, 50, 100\n",
    "items_per_page = 100\n",
    "\n",
    "#print(f\"Pick a range from 1 - {items_per_page} for where you want to start downloading\")\n",
    "start = 61\n",
    "\n",
    "params = go_to_page(page, items_per_page, start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "208ad6dc",
   "metadata": {},
   "source": [
    "# Scrapping\n",
    "\n",
    "Todo: \n",
    "fix tessarct\n",
    "set timer limit for driver\n",
    "Update parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7af1b550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting downloads at page:11\n",
      "---------------------------\n",
      "Time waiting: 330 seconds\n",
      "done waiting\n",
      "0.16666666666666666\n",
      "pdf: 1061, cell: A1062:D1062 title:The Experience of Being Unsheltered in Atlanta \n",
      "0.10638297872340426\n",
      "pdf: 1062, cell: A1063:D1063 title:Attachment Types and Classroom Behaviors of Children with Emotional Disturbance: Multiple-Case Study \n",
      "0.12121212121212122\n",
      "pdf: 1063, cell: A1064:D1064 title:The Use of Emotional Intelligence and Positive Emotions in Coping with Chronic Unemployment \n",
      "0.15163934426229508\n",
      "pdf: 1064, cell: A1065:D1065 title:Exploring the Relationship between Cognitive Appraisal and Perceived Stress in Individuals Who Engage in Physical Activity \n",
      "ack not found\n",
      "pdf: 1065, cell: A1066:D1066 title:The Role of Experience in the Use of Speech Cues by Budgerigars \n",
      "ack not found\n",
      "pdf: 1066, cell: A1067:D1067 title:Behavioral Health Medical Interpreters: Cluster Analysis of Vicarious Traumatization and Posttraumatic Growth \n",
      "0.15789473684210525\n",
      "pdf: 1067, cell: A1068:D1068 title:Effect of Culturally Based Arts Activities on Self-Efficacy, Self-Expression, and Achievement Motivation in Adolescent Inner-City Youth \n",
      "ack not found\n",
      "pdf: 1068, cell: A1069:D1069 title:A Phenomenological Study of Single Fathers of Children with Autism in Trinidad \n",
      "0.16272965879265092\n",
      "pdf: 1069, cell: A1070:D1070 title:Academic Success for the 21st Century Learner: Intrapersonal Intelligence and Resilience \n",
      "0.1414141414141414\n",
      "pdf: 1070, cell: A1071:D1071 title:Success of the African American Female Military Field Grade Officers in an Army Environment \n",
      "ack not found\n",
      "pdf: 1071, cell: A1072:D1072 title:The Role of Family Functioning and Perceived Parental Support in Suicidal and Non-suicidal Self-injurious Ethnic Minority Adolescents \n",
      "ack not found\n",
      "pdf: 1072, cell: A1073:D1073 title:Maladaptive personality and mental health treatment adherence in September 11<sup>th</sup> first responders \n",
      "0.09090909090909091\n",
      "pdf: 1073, cell: A1074:D1074 title:Lexical and Syntactic Influences on Structural Selection in Language Production \n",
      "0.16352201257861634\n",
      "pdf: 1074, cell: A1075:D1075 title:Situational demands and emotional significance during language processing \n",
      "0.14414414414414414\n",
      "pdf: 1075, cell: A1076:D1076 title:Impact of Adult Aging on Behavioral and Event-Related Potential (ERP) Effects of Lexical Competition in Spoken Word Recognition \n",
      "0.15270935960591134\n",
      "pdf: 1076, cell: A1077:D1077 title:Self adapted testing as formative assessment: Effects of feedback and scoring on engagement and performance \n",
      "0.14084507042253522\n",
      "pdf: 1077, cell: A1078:D1078 title:An Examination of the Relationship between Authenticity and Female Sexual Dysfunction \n",
      "0.15384615384615385\n",
      "pdf: 1078, cell: A1079:D1079 title:Tired, hungry, and grumpy: Understanding the direct and indirect relationships among child temperament, sleep problems, feeding styles, and weight outcomes \n",
      "0.16336633663366337\n",
      "pdf: 1079, cell: A1080:D1080 title:Alien, Illegal, Undocumented: Labeling, Context, and Worldview in the Immigration Debate and in the Lives of Undocumented Youth \n",
      "0.5279878971255674\n",
      "pdf: 1080, cell: A1081:D1081 title:Exploring the uses of cultural funds of knowledge among ethnic minority immigrant college students in their constructions of learning identities within a collaborative photovoice project \n",
      "Time waiting: 260 seconds\n"
     ]
    }
   ],
   "source": [
    "count,num_results, next_page, results = params[\"count\"], params['num_results'] , params[\"next_page\"] , params[\"results\" ]\n",
    "for i in range(1): \n",
    "    ''' In the event, I get an unexpected TimeoutError this loop will call the code below. However, it will only run the below twice '''\n",
    "    try:\n",
    "        Error = \"Does not exist in this dissertation\"\n",
    "        while count <= num_results:\n",
    "            print(f\"starting downloads at page:{page}\")\n",
    "            print(\"---------------------------\")\n",
    "\n",
    "            for index in range(start-1, len(results)): #O(n)\n",
    "                if index % 2 == 0:\n",
    "                    multiplier = np.random.randint(1,15,1)[0]\n",
    "                    print(f\"Time waiting: {multiplier } seconds\")\n",
    "                    sleep(multiplier * 10)\n",
    "                    print(\"done waiting\")\n",
    "                data = {'ack1':Error, 'ack2': Error}\n",
    "                cell = f\"A{count+1}:D{count+1}\"\n",
    "                result = results[index]\n",
    "                link = result.find(attrs = {\"contentArea\"}).find(\"a\")[\"href\"]\n",
    "                bs = getBs(link) #O(k)   \n",
    "                div = result.find_all(\"div\", attrs= {\"class\": \"truncatedResultsTitle\"})[0] #O(n)\n",
    "\n",
    "                data['title'] = \"\".join([str(i) for i in bs.find_all(\"h1\", attrs= {\"class\": \"documentTitle\"})[0].contents]) #O(k)\n",
    "                # find author\n",
    "                spans = result.find_all(\"span\", attrs= {\"class\": \"truncatedAuthor\"}) #O(n)\n",
    "                data['author'] = spans[0].contents[0].replace(\".\\xa0\\n\",\"\") \n",
    "                try:\n",
    "                    pdf = bs.find(attrs = {\"download\":\"ProQuestDocument.pdf\"})[\"href\"]\n",
    "                    ack1, ack2 = writeContent(pdf)\n",
    "                    data['ack1'], data['ack2'] = compare(ack1,ack2)\n",
    "                    send(data,cell,service)\n",
    "                    \n",
    "                except TypeError:           \n",
    "                    span = bs.find_all(lambda tag:findLink(tag)) #O(k)\n",
    "                    if(len(span) != 0):\n",
    "                        link = span.a[\"href\"]\n",
    "                        bs = getBs(link) #O(k)\n",
    "                        pdf = bs.find(attrs = {\"download\":\"ProQuestDocument.pdf\"})[\"href\"] #O(k)\n",
    "                        ack1, ack2 = writeContent(pdf) #O(w)\n",
    "                        data['ack1'], data['ack2'] = compare(ack1,ack2)\n",
    "                        send(data,cell,service)\n",
    "                    else:\n",
    "                        print(\"Couln't download PDF\")\n",
    "                        # find title\n",
    "                        send(data,cell,service)\n",
    "                        \n",
    "                except HttpError as exception:\n",
    "                    if len(data['ack2']) > 50000:\n",
    "                        data['ack2'] = 'Parser failed to parse correctly. please re-visit'\n",
    "                    if len(data['ack1']) > 50000:\n",
    "                        data['ack1'] = 'Parser failed to parse correctly. please re-visit'\n",
    "                    send(data,cell,service)\n",
    "\n",
    "                title = data['title']         \n",
    "                print(f'pdf: {count}, cell: {cell} title:{title} ') \n",
    "                count +=1\n",
    "                    \n",
    "            if len(next_page) == 1: # there is a next page\n",
    "                page+=1\n",
    "                print(f'going to next page')\n",
    "                next_page_link = next_page[0]['href']\n",
    "                bs = getBs(next_page_link)\n",
    "                results = bs.find_all(attrs= {\"class\": \"resultItem ltr\"}) \n",
    "                next_page = bs.find_all(\"a\", attrs = {\"title\" : \"Next Page\"} )\n",
    "                start = 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    except (TimeoutError , WebDriverException )as e:\n",
    "        print(e)\n",
    "        print(\"Trying again\")\n",
    "        del driver\n",
    "        driver = start_driver()\n",
    "        params = go_to_page(page, items_per_page, start)\n",
    "        count,num_results, next_page, results = params[\"count\"], params['num_results'] , params[\"next_page\"] , params[\"results\" ]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f599c33",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f812cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "77d4f4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 1061, 11)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, count, page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22141cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1061)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6cad463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5627701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "15926349cd38fea96ad86cae6289b5d11662f9306a0eee05fb693cc6e10af888"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
