{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de2feb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting driver\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver \n",
    "import signal\n",
    "import psutil\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from google.oauth2 import service_account\n",
    "from selenium.webdriver.common.by import By\n",
    "from googleapiclient.errors import HttpError\n",
    "from urllib3.exceptions import ProtocolError\n",
    "import csv\n",
    "import PyPDF2\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver import ChromeOptions\n",
    "web_pages = dict()\n",
    "import vlc\n",
    "import pdfplumber\n",
    "\n",
    "import pdb\n",
    "import regex as re\n",
    "# from PyPDF2 import PdfFileReader, PdfFileWriter\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import os.path\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "\n",
    "SAMPLE_SPREADSHEET_ID = '1kge8VOTe7oUNFagXmP2f3HdgI0j4-y2wOQlcTjuuTy0'\n",
    "MUSIC_PATH = \"/Users/oluwaseuncardoso/PythonStuff/AlarmNuclearMeltd HYP013301_preview.mp3\"\n",
    "USER_DATA_DIR = '/Users/oluwaseuncardoso/Desktop/Projects/Research-paper-parser/user_data'\n",
    "CHROME_DRIVER_PATH = \"/Users/oluwaseuncardoso/Downloads/chromedriver\"\n",
    "QUERY = \"STYPE(DISSERTATION) AND PD(2010-2016) AND DEP.X(PSYCHOLOGY) AND LA(ENGLISH) AND DG(PHD) AND (ULO(UNITED STATES))  NOT SU(CLINICAL)\"\n",
    "KEY_PATH = './Keys/simpeapis-ff416f56b1c4.json'\n",
    "\n",
    "\n",
    "\n",
    "SAMPLE_RANGE_NAME = 'A2:D7'\n",
    "p = vlc.MediaPlayer(MUSIC_PATH)\n",
    "\n",
    "\n",
    "def findLink(tag):\n",
    "    if tag.name == \"span\" and tag.a != None and tag.get(\"class\") != None:\n",
    "        cond = \"gateway.proquest.com\"\n",
    "        if tag[\"class\"][0] == 'subjectField-postProcessingHook' and cond in tag.a[\"href\"]:\n",
    "            return True\n",
    "    return False\n",
    "def findCapcha(tag):\n",
    "    if tag.name ==\"div\" and div.get(\"id\") == \"start\":\n",
    "        if tag.div.get(\"class\") == \"alert alert-info container captcha_alert_box\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def timeout_handler(num, stack):\n",
    "    print(\"Driver Unresponsive\")\n",
    "    #raise Exception(\"Driver Unresponsive\")\n",
    "    raise TimeoutError(\"Driver Unresponsive\")\n",
    "\n",
    "\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "def getBs(link):\n",
    "    \"\"\"\n",
    "    This function takes a proquest Url\n",
    "    and returns the beautiful soup object\n",
    "    \"\"\"\n",
    "    signal.alarm(10)\n",
    "    driver.get(link)\n",
    "    signal.alarm(0)\n",
    "    html = driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "    bs = BeautifulSoup(html, \"html5lib\")\n",
    "    capcha1 = bs.find_all(\"div\", attrs = {\"class\": \"alert alert-info container captcha_alert_box\"} )\n",
    "    playing = False\n",
    "    while(len(capcha1) != 0 ):\n",
    "        print(\"start playing alarm\")\n",
    "        p.play()\n",
    "        playing = True\n",
    "        sleep(5)\n",
    "        break\n",
    "    if(playing == True):\n",
    "        p.stop()\n",
    "    return bs\n",
    "\n",
    "\n",
    "def writeContent(pdf): # come here to change code\n",
    "    response = requests.get(pdf) \n",
    "    with open('./dissertation.pdf', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "        sleep(4)  \n",
    "    ack1 = findAck()\n",
    "    ack2 = findAck2()\n",
    "    return ack1, ack2\n",
    "\n",
    "def getAckFromLines(lines,ACK2=None):\n",
    "    # numeral numbers\n",
    "    regex1 = r\"[^\\w]+i$+|^i$|^1[^\\w]+|[^\\w]+1$+|^1$|\"\n",
    "    regex2 = r\"^ii[^\\w]+|[^\\w]+ii$+|^ii$|^2[^\\w]+|[^\\w]+2$+|^2$|\"\n",
    "    regex3 = r\"^iii[^\\w]+|[^\\w]+iii$+|^iii$|^3[^\\w]+|[^\\w]+3$+|^3$|\"\n",
    "    regex4 = r\"^iv[^\\w]+|[^\\w]+iv$+|^iv$|^4[^\\w]+|[^\\w]+4$+|^4$|\"\n",
    "    regex5 = r\"^v[^\\w]+|[^\\w]+v$+|^v$|^5[^\\w]+|[^\\w]+5$+|^5$|\"\n",
    "    regex6 = r\"^vi[^\\w]+|[^\\w]+vi$+|^vi$|^6[^\\w]+|[^\\w]+6$+|^6$|\"\n",
    "    regex7 = r\"^vii[^\\w]+|[^\\w]+vii$+|^vii$|^7[^\\w]+|[^\\w]+7$+|^7$|\"\n",
    "    regex8 = r\"^viii[^\\w]+|[^\\w]+viii$+|^viii$|^8[^\\w]+|[^\\w]+8$+|^8$|\"\n",
    "    regex9 = r\"^ix[^\\w]+|[^\\w]+ix$+|^ix$|^9[^\\w]+|[^\\w]+9$+|^9$|\"\n",
    "    regex10 = r\"^x[^\\w]+|[^\\w]+x$+|^x$|^10[^\\w]+|[^\\w]+10$+|^10$\"\n",
    "    regex = regex1+regex2+regex3+regex4+regex5+regex6+regex7+regex8+regex9+regex10\n",
    "    \n",
    "    ack_index = -1\n",
    "    if ACK2 != None:\n",
    "        for i in range(len(lines)):\n",
    "            if re.search(ACK2, lines[i],  re.IGNORECASE):\n",
    "                ack_index = i\n",
    "                break\n",
    "        lines = lines[ack_index+1:]\n",
    "\n",
    "    # remove every after page number(if page number exists)\n",
    "    end_line = len(lines) \n",
    "    half_page_line = len(lines)//2 # the page number will must likes be between the bottom and half the page\n",
    "    last_page_line = len(lines)-1\n",
    "    for i in range(last_page_line, half_page_line, -1):\n",
    "        if re.search(regex, lines[i]):\n",
    "            #pdb.set_trace()\n",
    "            end_line = i\n",
    "            break\n",
    "    return lines[:end_line]\n",
    "    \n",
    "def findAck(pdf_path = \"./dissertation.pdf\",from_page = 0,to_page = 29):\n",
    "    \"\"\"\n",
    "    create a new pdf file from a subsection from pdf\n",
    "    from_page(int): Where to start. Starts from 0.\n",
    "    to_page(int): Where to end(inclusive).\n",
    "    \"\"\"\n",
    "    pdf = PdfReader(pdf_path)\n",
    "    num_pages = len(pdf.pages)\n",
    "    if to_page > num_pages:\n",
    "        to_page = num_pages-1\n",
    "    ack_found = None\n",
    "    ack_extracted, content_extracted = False, False\n",
    "    acknowledgment = \"Acknoledgement not present in file\"\n",
    "    for page_num in range(from_page, to_page):\n",
    "        try: \n",
    "            pdfWriter = PdfWriter()\n",
    "            pdfWriter.add_page(pdf.pages[page_num])   \n",
    "            with open(f'./temp.pdf', 'wb') as temp:\n",
    "                pdfWriter.write(temp)\n",
    "            temp.close()\n",
    "            content = convertToString().strip()\n",
    "        except PyPDF2.errors.PdfReadError:\n",
    "            return {\"bool\" : False, \"content\" :\"PDF Broken\"}\n",
    "        if(ack_found == None):\n",
    "            ACK = r'ack[n]?owledg[e]?ment[s]?\\s*\\n'\n",
    "            ack_found = re.search(ACK,content, re.IGNORECASE)\n",
    "        if(ack_found != None and ack_extracted == False):\n",
    "            #pdb.set_trace()\n",
    "            ACK2 = ack_found[0].split(\"\\n\")[0] \n",
    "            #find the index to find the index in lines\n",
    "            \n",
    "            try:\n",
    "                ack_index = content.split(\"\\n\").index(ACK2)\n",
    "            except ValueError:\n",
    "                return {\"bool\" : False , \"content\" : \"Error: Special case please review\"}\n",
    "            \n",
    "            if ack_index > 4:\n",
    "                ack_found = None\n",
    "            else:\n",
    "                acknowledgment =  content.strip()     \n",
    "                lines = acknowledgment.split('\\n')          \n",
    "                lines = getAckFromLines(lines,ACK2)\n",
    "                acknowledgment = \"\\n\".join(lines)\n",
    "                ack_extracted  = True\n",
    "            continue # start loop again\n",
    "        if(ack_extracted == True and content_extracted == False):\n",
    "            #pdb.set_trace()\n",
    "            lines = content.split('\\n')\n",
    "            next_title = lines[ack_index]\n",
    "            words_of_next_title = next_title.split(\" \")\n",
    "            words_of_next_title = [i for i in words_of_next_title if i not in [\"\",\" \"]]\n",
    "            #pdb.set_trace()\n",
    "            there_is_no_title = len(words_of_next_title) > 6\n",
    "            if (there_is_no_title):   \n",
    "                lines = content.strip().split('\\n')          \n",
    "                lines = getAckFromLines(lines)\n",
    "                acknowledgment += \"\\n\".join(lines)\n",
    "                last_word = lines[-1]                    \n",
    "            else:\n",
    "                content_extracted = True\n",
    "        if content_extracted == True:            \n",
    "            return {\"bool\" : True, \"content\" :acknowledgment}\n",
    "        \n",
    "       \n",
    "    return {\"bool\" : False, \"content\" :\"ACKNOTFOUND\"}\n",
    "\n",
    "def convertToString():\n",
    "    images = convert_from_path(\"./temp.pdf\")\n",
    "    content = \"\"\n",
    "    images[0].save(f\"./temp.jpg\",\"JPEG\")\n",
    "    content = pytesseract.image_to_string(f\"./temp.jpg\")\n",
    "    return content\n",
    "import numpy as np\n",
    "def Levenshtein(r, h):\n",
    "    \"\"\"                                                                         \n",
    "    Calculation of WER with Levenshtein distance.                               \n",
    "                                                                                \n",
    "    Works only for iterables up to 254 elements (uint8).                        \n",
    "    O(nm) time ans space complexity.                                            \n",
    "                                                                                \n",
    "    Parameters                                                                  \n",
    "    ----------                                                                  \n",
    "    r : list of strings                                                                    \n",
    "    h : list of strings                                                                   \n",
    "                                                                                \n",
    "    Returns                                                                     \n",
    "    -------                                                                     \n",
    "    (WER, nS, nI, nD): (float, int, int, int) WER, number of substitutions, insertions, and deletions respectively\n",
    "                                                                                \n",
    "    Examples                                                                    \n",
    "    --------                                                                    \n",
    "    >>> wer(\"who is there\".split(), \"is there\".split())                         \n",
    "    0.333 0 0 1                                                                           \n",
    "    >>> wer(\"who is there\".split(), \"\".split())                                 \n",
    "    1.0 0 0 3                                                                           \n",
    "    >>> wer(\"\".split(), \"who is there\".split())      #ask in Pia\n",
    "    Inf 0 3 0                                                                           \n",
    "    \"\"\"\n",
    "\n",
    "    n = len(r) # The number of words in REF\n",
    "    m = len(h) # The number of words in HYP\n",
    "    R = np.zeros((n+1,m+1))\n",
    "    B = np.zeros((n+1,m+1))\n",
    "\n",
    "    #for all i,j s.t.  i = 0 or  j = 0,\tset\tR[i,j] ← max (i,j) end\n",
    "    R[0,:] = np.arange(m+1)\n",
    "    R[:,0] = np.arange(n+1)\n",
    "    # i think we should do this aswell\n",
    "    up = 0\n",
    "    left = 1\n",
    "    up_left = 2\n",
    "    up_left2 = 3\n",
    "    B[0,:] = left\n",
    "    B[:,0] = up\n",
    "    B[0,0] = up\n",
    "    for i in range(1,n+1):\n",
    "        for j in range(1,m+1):\n",
    "            dele = R[i - 1, j] + 1 # delete\n",
    "            sub = R[i - 1, j - 1] + (1,0)[r[i-1] == h[j-1]] #substitute #NOTE look at this\n",
    "            ins = R[i, j-1] + 1 #insert\n",
    "\n",
    "            R[i,j] = min(dele,sub,ins)\n",
    "            if R[i,j] == dele:\n",
    "                B[i , j] = up\n",
    "            elif R[i , j] == ins:\n",
    "                B[i,j] = left\n",
    "            else:\n",
    "                B[i,j] = (up_left, up_left2 )[r[i-1] == h[j-1]]\n",
    "    i,j = n,m\n",
    "    nSub,nDel,nIns = 0, 0, 0\n",
    "    transversal = True\n",
    "    while transversal == True:\n",
    "        path = B[i,j]\n",
    "        if i <=  0 and j <=0:\n",
    "            transversal = False\n",
    "            break\n",
    "        if path == up_left:\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "            nSub += 1\n",
    "        elif path == left:\n",
    "            j -= 1\n",
    "            nIns +=1\n",
    "        elif path == up:\n",
    "            i -= 1\n",
    "            nDel +=1\n",
    "        else: # correct\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "    return R[n,m]/n, nSub, nIns, nDel        \n",
    "    \n",
    "def getAcknoledgement(page,pages):\n",
    "    \"\"\"\n",
    "    Rule:\n",
    "    1. Look for the first occurcance of Acknowledgments and a new line.\n",
    "    2. Caputure the content until you hit the title of the next section  \n",
    "    #ASSUMPTIONS that the dissertations need to follow.\n",
    "    #1. Every dissertation starts it's acknoledgement with the word \"Acknoledgement\" as the only word in it's title\n",
    "    #2. No two titles can occupy the same page. \n",
    "    #3. The acknoledgement starts with a capital letter (i.e Acknoledgement)\n",
    "    #4. The title acknoledgemen may or may not end an s.\n",
    "    #5. If a line has 1 to 6 words in it. Then it's a title of a new section.\n",
    "    #6. Titles are located on the same location on each page.i.e top and centre\n",
    "    #7. Each word is of average size(6 chars long)\n",
    "    #8. Target titles can either be Acknowledgments or acknowledg(e)ments\n",
    "    #9. The Acknoledgemts page doesn't have picture/images\n",
    "    #10. Acknoledgements are never in the first page\n",
    "    Params: \n",
    "    page : pdfplumber.page.Page\n",
    "    returns(String) :The content of the abstraction\n",
    "    \"\"\"    \n",
    "    initial_index = page.page_number -1\n",
    "    content = page.extract_text()\n",
    "    if(content is None): return None\n",
    "    content = content.strip()\n",
    "    ACK = r'ack[n]?owledg[e]?ment[s]?\\s*\\n'\n",
    "    ack_found = re.search(ACK,content, re.IGNORECASE)\n",
    "    title_len = len(ACK)\n",
    "    acknowledgment = None\n",
    "    #if Acknowledgement is the title extract it else continue\n",
    "    if(ack_found):\n",
    "        #pdb.set_trace()\n",
    "        ACK2 = ack_found[0].split(\"\\n\")[0] \n",
    "        acknowledgment =  content.strip()     \n",
    "        lines = acknowledgment.split('\\n')          \n",
    "        lines = getAckFromLines(lines,ACK2)\n",
    "        acknowledgment = \"\\n\".join(lines)        \n",
    "        stop_not_true = True\n",
    "        #find the index to find the index in lines\n",
    "        try:\n",
    "            ack_line = content.strip().split(\"\\n\")\n",
    "            ack_index = ack_line.index(ACK2)\n",
    "            ack_line = [ack_line[i] for i in range(ack_index+1) if i not in [\"\", \" \"]]\n",
    "            ack_index = ack_line.index(ACK2)\n",
    "        except ValueError:\n",
    "            return \"Error: Special case please revuew\"\n",
    "        counter = 1\n",
    "        while(stop_not_true):\n",
    "            next_page = counter + initial_index\n",
    "            #pdb.set_trace()\n",
    "            page = pages[next_page]\n",
    "            content = page.extract_text().lower().lstrip()\n",
    "            lines = content.split('\\n')\n",
    "            try:\n",
    "                next_title = lines[ack_index]\n",
    "            except:\n",
    "                next_title = \"\"\n",
    "            words_of_next_title = next_title.split(\" \")\n",
    "            words_of_next_title = [i for i in words_of_next_title if i not in [\"\",\" \"]]\n",
    "            #pdb.set_trace()\n",
    "            there_is_no_title = len(words_of_next_title) > 6\n",
    "            if (there_is_no_title):   \n",
    "                lines = content.strip().split('\\n')          \n",
    "                lines = getAckFromLines(lines)\n",
    "                acknowledgment += \"\\n\".join(lines)\n",
    "                last_word = lines[-1]                    \n",
    "                counter+=1\n",
    "            else:\n",
    "                stop_not_true = False\n",
    "    return acknowledgment\n",
    "\n",
    "def findAck2(pdf_path = \"./dissertation.pdf\",from_page = 0,to_page = 29):\n",
    "    with pdfplumber.open(pdf_path, strict_metadata=True) as pdf:\n",
    "        pages = iter(pdf.pages)\n",
    "        count = 0\n",
    "        \n",
    "        for page in pages:\n",
    "            count +=1\n",
    "            if count >= from_page: \n",
    "                content = page.extract_text()        \n",
    "                acknowledgement = getAcknoledgement(page, pdf.pages)    \n",
    "                if(acknowledgement != None):\n",
    "                    if(\"(cid:\" in acknowledgement):\n",
    "                        acknowledgement = \"ERROR: It contains embedded fonts. This requires revisitation.\"\n",
    "                        \n",
    "                    return {\"bool\": True ,  \"content\": acknowledgement}\n",
    "            if page.page_number == to_page:\n",
    "                print(\"ack not found\")\n",
    "                return {\"bool\": False, \"content\":\"ACKNOTFOUND\"  }\n",
    "            \n",
    "\n",
    "def compare(ack1,ack2):\n",
    "    if(ack1['bool'] == True and ack2['bool']  == True ):\n",
    "        content1 = ack1[\"content\"].replace(\"\\n\", \"\").strip().split()\n",
    "        content2 = ack2[\"content\"].replace(\"\\n\", \"\").strip().split()\n",
    "        WER = Levenshtein(content1, content2)[0]\n",
    "        print(WER)\n",
    "        if(WER < 0.2):\n",
    "            return ack1[\"content\"], \"N/A: same as 1st parser\"          \n",
    "    return ack1[\"content\"] , ack2[\"content\"]\n",
    "\n",
    "\n",
    "def main_alt():\n",
    "    \"\"\"Shows basic usage of the Sheets API.\n",
    "    Prints values from a sample spreadsheet.\n",
    "    \"\"\"\n",
    "    # If modifying these scopes, delete the file token.json.\n",
    "    SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "    creds = None\n",
    "    # The file token.json stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())            \n",
    "    return creds\n",
    "\n",
    "def main():\n",
    "    SCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/cloud-platform']\n",
    "    SERVICE_ACCOUNT_FILE = KEY_PATH\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "            SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    return credentials\n",
    "    \n",
    "creds = main()\n",
    "service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "def send(data, cells, service, value_range_body = None):\n",
    "    if value_range_body == None:\n",
    "\n",
    "        value_range_body = {\n",
    "            \"majorDimension\" : \"ROWS\",\n",
    "            \"values\" : \n",
    "                [\n",
    "                [data['author'],data['title'], data['ack1'],data['ack2']],\n",
    "                ]\n",
    "        }\n",
    "\n",
    "\n",
    "    # have a program that send s batch as oppossed to just one\n",
    "    sheet = service.spreadsheets()\n",
    "    request = sheet.values().update(spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "                          range=cells,\n",
    "                          valueInputOption ='RAW',\n",
    "                          includeValuesInResponse = True,\n",
    "                          body = value_range_body )\n",
    "    try:\n",
    "        return request.execute()\n",
    "    except (ConnectionResetError, OSError):\n",
    "        print(\"ConnectionResetError\")\n",
    "        creds = main()\n",
    "        service = build('sheets', 'v4', credentials=creds)\n",
    "        return send(data, cells, service)\n",
    "\n",
    "def get( cells, service):\n",
    "    \n",
    "\n",
    "    # have a program that send s batch as oppossed to just one\n",
    "    sheet = service.spreadsheets()\n",
    "    request = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "                          range=cells)\n",
    "    try:\n",
    "        return request.execute()\n",
    "    except (ConnectionResetError, OSError):\n",
    "        print(\"ConnectionResetError\")\n",
    "        creds = main()\n",
    "        service = build('sheets', 'v4', credentials=creds)\n",
    "        return get(cells, service)\n",
    "\n",
    "def get_meta(result):\n",
    "    author = result.find_all(attrs= {\"class\": \"truncatedAuthor\"})[0].contents[0]\n",
    "    end = author.index('.')\n",
    "    author = author[0:end+1]\n",
    "    University  = result.find_all(attrs= {\"class\": \"dissertpub\"})[0].contents[0].contents[0]\n",
    "    University = re.sub(r\"\\d|\\u2009|ProQuest Dissertations Publishing|,|\\.\",\"\",University).rstrip()\n",
    "    return author, University\n",
    "\n",
    "def check(a, b):\n",
    "    a,b= a[0], b[0]\n",
    "    end = b.index('.')\n",
    "    b = b[0:end+1]\n",
    "    if a != b:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "    \n",
    "def delete_driver(driver):\n",
    "    p = psutil.Process(driver.service.process.pid)\n",
    "    children = p.children(recursive=True)\n",
    "    for process in children:\n",
    "        pid = process.pid\n",
    "        os.kill(pid, signal.SIGKILL)\n",
    "\n",
    "def start_driver():\n",
    "    chromeOptions = ChromeOptions() \n",
    "    chromeOptions.add_argument(f\"--user-data-dir={USER_DATA_DIR}\")\n",
    "    chromeOptions.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chromeOptions.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    s = Service(CHROME_DRIVER_PATH)\n",
    "\n",
    "    driver = webdriver.Chrome(service = s, options = chromeOptions)\n",
    "    driver.set_page_load_timeout(60)\n",
    "    #12-i#VFZ4eVWZ8fq6V\n",
    "    signal.alarm(120)\n",
    "    print(\"starting driver\")\n",
    "    driver.get(\"https://www.proquest.com/\")\n",
    "    signal.alarm(0)\n",
    "    return driver\n",
    "    \n",
    "driver = start_driver()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b4cb9a6",
   "metadata": {},
   "source": [
    "# Go to specific page\n",
    "Play the tab below and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ef7a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix this problem\n",
    "def go_to_page(page, items_per_page, start, driver=driver):\n",
    "    sleep(5)\n",
    "    try:\n",
    "        elem = driver.find_element(By.ID,\"queryTermField\")  \n",
    "    except  NoSuchElementException:\n",
    "        elem = driver.find_element(By.ID,\"searchTerm\")\n",
    "    elem.clear()\n",
    "    elem.send_keys(QUERY)\n",
    "    elem.send_keys(Keys.RETURN)\n",
    "    sleep(13)\n",
    "\n",
    "    dropdown = Select(driver.find_element(By.ID,\"itemsPerPage\"))\n",
    "    dropdown.select_by_value(str(items_per_page))\n",
    "    sleep(15)\n",
    "\n",
    "    search = driver.find_element(By.ID,\"pageNbrField\") \n",
    "    search.clear()\n",
    "    search.send_keys(str(page))\n",
    "    search.send_keys(Keys.RETURN)\n",
    "\n",
    "    sleep(15)\n",
    "\n",
    "    html = driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "    bs = BeautifulSoup(html, \"html5lib\")\n",
    "    results = bs.find_all(attrs= {\"class\": \"resultItem ltr\"})\n",
    "    h1 = bs.find_all( \"h1\")[0]\n",
    "    num_results = h1.contents[0]\n",
    "    num_results = num_results.strip()\n",
    "    num_results = num_results.replace(',', \"\")\n",
    "    num_results = re.search(r'[0-9]*', num_results)\n",
    "    num_results = int(num_results[0])\n",
    "    next_page = bs.find_all(\"a\", attrs = {\"title\" : \"Next Page\"} )\n",
    "    count = (page -1) * items_per_page + start\n",
    "    return {'count' : count, 'num_results' : num_results, \"results\" : results , \"next_page\" : next_page}\n",
    "    \n",
    "#print(\"What page do you want to go to: \")\n",
    "page = 55 #int(input())\n",
    "\n",
    "# How many pages per page do you want to see 10, 20, 50, 100\n",
    "items_per_page = 100\n",
    "\n",
    "#print(f\"Pick a range from 1 - {items_per_page} for where you want to start downloading\")\n",
    "start = 1\n",
    "# Todo write code that will reliably wait until an action is done\n",
    "params = go_to_page(page, items_per_page, start)\n",
    "\n",
    "count,num_results, next_page, results = params[\"count\"], params['num_results'] , params[\"next_page\"] , params[\"results\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3f140ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.proquest.com/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "208ad6dc",
   "metadata": {},
   "source": [
    "# Scrapping\n",
    "\n",
    "Todo:  \n",
    "I you're going to use this script again please. Learn this first:\n",
    "\n",
    "https://realpython.com/async-io-python/#:~:text=Asynchronous%20routines%20are%20able%20to,look%20and%20feel%20of%20concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af1b550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting downloads at page:55\n",
      "---------------------------\n",
      "everything is goood\n",
      "going to next page\n",
      "starting downloads at page:56\n",
      "---------------------------\n",
      "everything is goood\n",
      "going to next page\n",
      "starting downloads at page:57\n",
      "---------------------------\n",
      "everything is goood\n",
      "going to next page\n",
      "starting downloads at page:58\n",
      "---------------------------\n",
      "everything is goood\n",
      "going to next page\n",
      "starting downloads at page:59\n",
      "---------------------------\n",
      "everything is goood\n",
      "going to next page\n",
      "starting downloads at page:60\n",
      "---------------------------\n",
      "everything is goood\n",
      "going to next page\n",
      "starting downloads at page:61\n",
      "---------------------------\n",
      "everything is goood\n",
      "going to next page\n",
      "starting downloads at page:62\n",
      "---------------------------\n",
      "everything is goood\n",
      "going to next page\n",
      "starting downloads at page:63\n",
      "---------------------------\n",
      "everything is goood\n",
      "going to next page\n",
      "starting downloads at page:64\n",
      "---------------------------\n",
      "everything is goood\n",
      "going to next page\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m page\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgoing to next page\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m next_page_link \u001b[39m=\u001b[39m next_page[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     36\u001b[0m bs \u001b[39m=\u001b[39m getBs(next_page_link)\n\u001b[1;32m     37\u001b[0m results \u001b[39m=\u001b[39m bs\u001b[39m.\u001b[39mfind_all(attrs\u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mresultItem ltr\u001b[39m\u001b[39m\"\u001b[39m}) \n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "Error = \"Does not exist in this dissertation\"\n",
    "data = {'ack1':Error, 'ack2': Error}\n",
    "\n",
    "for i in range(2): \n",
    "    ''' In the event, I get an unexpected TimeoutError this loop will call the code below. However, it will only run the below twice '''\n",
    "    try:\n",
    "        \n",
    "        while count <= num_results:\n",
    "            print(f\"starting downloads at page:{page}\")\n",
    "            print(\"---------------------------\")\n",
    "\n",
    "            \n",
    "            cells = f\"A{count+1}:A{count+100}\"\n",
    "            x = map(get_meta, results)\n",
    "            author_uni = list(x)\n",
    "            values = get( cells, service)['values']\n",
    "            array = np.array(author_uni)\n",
    "            authors = array[:,0].tolist()\n",
    "            universities = array[:,1].tolist()\n",
    "\n",
    "            not_equal = sum(list(map(check, author_uni, values))) != 0\n",
    "            if not_equal:\n",
    "                value_range_body = {\n",
    "                                \"majorDimension\" : \"COLUMNS\",\n",
    "                                \"values\" : \n",
    "                                    [\n",
    "                                    universities\n",
    "                                    ]\n",
    "                            }\n",
    "                send([],f\"E{count+1}:E{count+100}\",service,value_range_body)\n",
    "\n",
    "            else:\n",
    "                print(\"everything is goood\")\n",
    "                page+=1\n",
    "                print(f'going to next page')\n",
    "                next_page_link = next_page[0]['href']\n",
    "                bs = getBs(next_page_link)\n",
    "                results = bs.find_all(attrs= {\"class\": \"resultItem ltr\"}) \n",
    "                next_page = bs.find_all(\"a\", attrs = {\"title\" : \"Next Page\"} )\n",
    "                start = 1\n",
    "                count = (page -1) * items_per_page + start\n",
    "                continue\n",
    "            \n",
    "            for index in range(start-1, len(results)): #O(n)\n",
    "\n",
    "                authors_alligned = check(author_uni[index], values[index]) == 0\n",
    "                cell = f\"A{count+1}:D{count+1}\"\n",
    "                if not authors_alligned :                       \n",
    "                    print(f\"Processing: {cell}\")\n",
    "                    result = results[index]\n",
    "                    link = result.find(attrs = {\"contentArea\"}).find(\"a\")[\"href\"]\n",
    "                    bs = getBs(link) #O(k)   \n",
    "                    div = result.find_all(\"div\", attrs= {\"class\": \"truncatedResultsTitle\"})[0] #O(n)\n",
    "\n",
    "                    data['title'] = \"\".join([str(i) for i in bs.find_all(\"h1\", attrs= {\"class\": \"documentTitle\"})[0].contents]) #O(k)\n",
    "                    # find author\n",
    "                    spans = result.find_all(\"span\", attrs= {\"class\": \"truncatedAuthor\"}) #O(n)\n",
    "                    data['author'] = spans[0].contents[0].replace(\".\\xa0\\n\",\"\") \n",
    "                    try:\n",
    "                        pdf = bs.find(attrs = {\"download\":\"ProQuestDocument.pdf\"})[\"href\"]\n",
    "                        ack1, ack2 = writeContent(pdf)\n",
    "                        data['ack1'], data['ack2'] = compare(ack1,ack2)\n",
    "                        send(data,cell,service)\n",
    "\n",
    "                    except TypeError:           \n",
    "                        span = bs.find_all(lambda tag:findLink(tag)) #O(k)\n",
    "                        if(len(span) != 0):\n",
    "                            link = span.a[\"href\"]\n",
    "                            bs = getBs(link) #O(k)\n",
    "                            pdf = bs.find(attrs = {\"download\":\"ProQuestDocument.pdf\"})[\"href\"] #O(k)\n",
    "                            ack1, ack2 = writeContent(pdf) #O(w)\n",
    "                            data['ack1'], data['ack2'] = compare(ack1,ack2)\n",
    "                            send(data,cell,service)\n",
    "                        else:\n",
    "                            print(\"Couln't download PDF\")\n",
    "                            # find title\n",
    "                            send(data,cell,service)\n",
    "                            \n",
    "                    except HttpError as exception:\n",
    "                        if len(data['ack2']) > 50000:\n",
    "                            data['ack2'] = 'Parser failed to parse correctly. please re-visit'\n",
    "                        if len(data['ack1']) > 50000:\n",
    "                            data['ack1'] = 'Parser failed to parse correctly. please re-visit'\n",
    "                        send(data,cell,service)\n",
    "\n",
    "                    title = data['title']         \n",
    "                    print(f'pdf: {count}, cell: {cell} title:{title} ') \n",
    "                count +=1\n",
    "                  \n",
    "            if len(next_page) == 1: # there is a next page\n",
    "                page+=1\n",
    "                print(f'going to next page')\n",
    "                next_page_link = next_page[0]['href']\n",
    "                bs = getBs(next_page_link)\n",
    "                results = bs.find_all(attrs= {\"class\": \"resultItem ltr\"}) \n",
    "                next_page = bs.find_all(\"a\", attrs = {\"title\" : \"Next Page\"} )\n",
    "                start = 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    except (ProtocolError, TimeoutError) as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        ## kill all the chrome processes that is spawned from driver\n",
    "        delete_driver(driver)\n",
    "        sleep(5)\n",
    "        del driver\n",
    "        sleep(5)\n",
    "        print(\"Trying again\")\n",
    "        driver = start_driver()\n",
    "        params = go_to_page(page, items_per_page, start, driver)\n",
    "        count,num_results, next_page, results = params[\"count\"], params['num_results'] , params[\"next_page\"] , params[\"results\" ]\n",
    "        not_100_per_page = len(results) != 100\n",
    "        if not_100_per_page:\n",
    "            print(\"Number of pdfs per page not what is expected\")\n",
    "            break    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f599c33",
   "metadata": {},
   "source": [
    "Universities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f812cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count,num_results, next_page, results = params[\"count\"], params['num_results'] , params[\"next_page\"] , params[\"results\" ]\n",
    "\n",
    "cells = f\"A{count+1}:A{count+100}\"\n",
    "x = map(get_meta, results)\n",
    "author_uni = list(x)\n",
    "values = get( cells, service)['values']\n",
    "array = np.array(author_uni)\n",
    "authors = array[:,0].tolist()\n",
    "universities = array[:,1].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2709229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(authors)):\n",
    "    print(check(author_uni[i], values[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f47304f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 99\n",
    "check(author_uni[index], values[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77d4f4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Yang, Yueran.', ['Yang, Yueran.\\u2009\\n'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors[index], values[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22141cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b= a[0], b[0]\n",
    "end = b.index('.')\n",
    "b = b[0:end+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cad463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = map(get_meta, results)\n",
    "author_uni = list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0929be94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good: 1\n",
      "going to next page: 2\n",
      "Good: 2\n",
      "going to next page: 3\n",
      "Good: 3\n",
      "going to next page: 4\n",
      "Good: 4\n",
      "going to next page: 5\n",
      "error 1\n",
      "going to next page: 6\n",
      "Good: 5\n",
      "going to next page: 7\n",
      "error 2\n",
      "going to next page: 8\n",
      "error 3\n",
      "going to next page: 9\n",
      "Good: 6\n",
      "going to next page: 10\n",
      "Good: 7\n",
      "going to next page: 11\n",
      "error 4\n",
      "going to next page: 12\n",
      "error 5\n",
      "going to next page: 13\n",
      "error 6\n",
      "going to next page: 14\n",
      "Good: 8\n",
      "going to next page: 15\n",
      "Good: 9\n",
      "going to next page: 16\n",
      "Good: 10\n",
      "going to next page: 17\n",
      "Good: 11\n",
      "going to next page: 18\n",
      "Good: 12\n",
      "going to next page: 19\n",
      "Good: 13\n",
      "going to next page: 20\n",
      "Good: 14\n",
      "going to next page: 21\n",
      "Good: 15\n",
      "going to next page: 22\n",
      "Good: 16\n",
      "going to next page: 23\n",
      "Good: 17\n",
      "going to next page: 24\n",
      "Good: 18\n",
      "going to next page: 25\n",
      "Good: 19\n",
      "going to next page: 26\n",
      "Good: 20\n",
      "going to next page: 27\n",
      "Good: 21\n",
      "going to next page: 28\n",
      "Good: 22\n",
      "going to next page: 29\n",
      "error 7\n",
      "going to next page: 30\n",
      "Good: 23\n",
      "going to next page: 31\n",
      "Good: 24\n",
      "going to next page: 32\n",
      "Good: 25\n",
      "going to next page: 33\n",
      "Good: 26\n",
      "going to next page: 34\n",
      "Good: 27\n",
      "going to next page: 35\n",
      "Good: 28\n",
      "going to next page: 36\n",
      "Good: 29\n",
      "going to next page: 37\n",
      "Good: 30\n",
      "going to next page: 38\n",
      "Good: 31\n",
      "going to next page: 39\n",
      "Good: 32\n",
      "going to next page: 40\n",
      "Good: 33\n",
      "going to next page: 41\n",
      "Good: 34\n",
      "going to next page: 42\n",
      "Good: 35\n",
      "going to next page: 43\n",
      "Good: 36\n",
      "going to next page: 44\n",
      "Good: 37\n",
      "going to next page: 45\n",
      "Good: 38\n",
      "going to next page: 46\n",
      "Good: 39\n",
      "going to next page: 47\n",
      "Good: 40\n",
      "going to next page: 48\n",
      "Good: 41\n",
      "going to next page: 49\n",
      "Good: 42\n",
      "going to next page: 50\n",
      "Good: 43\n",
      "going to next page: 51\n",
      "Good: 44\n",
      "going to next page: 52\n",
      "Good: 45\n",
      "going to next page: 53\n",
      "Good: 46\n",
      "going to next page: 54\n",
      "Good: 47\n",
      "going to next page: 55\n",
      "error 8\n",
      "going to next page: 56\n",
      "Good: 48\n",
      "going to next page: 57\n",
      "Good: 49\n",
      "going to next page: 58\n",
      "Good: 50\n",
      "going to next page: 59\n",
      "Good: 51\n",
      "going to next page: 60\n",
      "Good: 52\n",
      "going to next page: 61\n",
      "Good: 53\n",
      "going to next page: 62\n",
      "Good: 54\n",
      "going to next page: 63\n",
      "Good: 55\n",
      "going to next page: 64\n",
      "Good: 56\n"
     ]
    }
   ],
   "source": [
    "count = 2\n",
    "_,num_results, next_page, results = params[\"count\"], params['num_results'] , params[\"next_page\"] , params[\"results\" ]\n",
    "error = 0\n",
    "good = 0\n",
    "while 1:\n",
    "    cells = f\"A{count}:A{count+99}\"\n",
    "    x = map(get_meta, results)\n",
    "    author_uni = list(x)\n",
    "    values = get( cells, service)['values']\n",
    "    array = np.array(author_uni)\n",
    "    authors = array[:,0].tolist()\n",
    "    universities = array[:,1].tolist()\n",
    "\n",
    "    equal = sum(list(map(check, author_uni, values))) == 0\n",
    "    if equal:\n",
    "        good +=1\n",
    "        print(f\"Good: {good}\")\n",
    "        value_range_body = {\n",
    "            \"majorDimension\" : \"COLUMNS\",\n",
    "            \"values\" : \n",
    "                [\n",
    "                universities\n",
    "                ]\n",
    "        }\n",
    "        send([],f\"E{count}:E{count+99}\",service,value_range_body)\n",
    "\n",
    "        value_range_body = {\n",
    "            \"majorDimension\" : \"COLUMNS\",\n",
    "            \"values\" : \n",
    "                [\n",
    "                authors\n",
    "                ]\n",
    "        }\n",
    "        send([],f\"A{count}:A{count+99}\",service,value_range_body)\n",
    "    else: # sanity checks # set all one hundred rows to error\n",
    "        error += 1\n",
    "        print(f\"error {error}\")\n",
    "        value_range_body = {\n",
    "            \"majorDimension\" : \"COLUMNS\",\n",
    "            \"values\" : \n",
    "                [\n",
    "                [\"Error re-process this row\"] *100\n",
    "                ]\n",
    "        }\n",
    "        send([],f\"E{count}:E{count+99}\",service,value_range_body)\n",
    "\n",
    "    if len(next_page) == 1: # there is a next page\n",
    "                page+=1\n",
    "                print(f'going to next page: {page}')\n",
    "                next_page_link = next_page[0]['href']\n",
    "                bs = getBs(next_page_link)\n",
    "                results = bs.find_all(attrs= {\"class\": \"resultItem ltr\"}) \n",
    "                next_page = bs.find_all(\"a\", attrs = {\"title\" : \"Next Page\"} )\n",
    "                count+= 100\n",
    "    else:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3eb4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#write a function that generates the cells that contain correctly aligned rows and cells that don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c371ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[000000013ef30c50] auhal audio output error: AudioObjectAddPropertyListener failed, device id 79, prop: [atfp], OSStatus: 1852797029\n",
      "[000000013ef30c50] auhal audio output error: AudioObjectAddPropertyListener failed, device id 50, prop: [atfp], OSStatus: 1852797029\n",
      "[000000011e25cba0] auhal audio output error: AudioObjectAddPropertyListener failed, device id 79, prop: [atfp], OSStatus: 1852797029\n",
      "[000000011e25cba0] auhal audio output error: AudioObjectAddPropertyListener failed, device id 50, prop: [atfp], OSStatus: 1852797029\n",
      "[000000013ef30c50] auhal audio output error: AudioObjectAddPropertyListener failed, device id 156, prop: [atfp], OSStatus: 1852797029\n",
      "[000000013ef30c50] auhal audio output error: AudioObjectAddPropertyListener failed, device id 79, prop: [atfp], OSStatus: 1852797029\n",
      "[000000013ef30c50] auhal audio output error: AudioObjectAddPropertyListener failed, device id 50, prop: [atfp], OSStatus: 1852797029\n",
      "[000000011e25cba0] auhal audio output error: AudioObjectAddPropertyListener failed, device id 156, prop: [atfp], OSStatus: 1852797029\n",
      "[000000011e25cba0] auhal audio output error: AudioObjectAddPropertyListener failed, device id 79, prop: [atfp], OSStatus: 1852797029\n",
      "[000000011e25cba0] auhal audio output error: AudioObjectAddPropertyListener failed, device id 50, prop: [atfp], OSStatus: 1852797029\n",
      "[000000013ef30c50] auhal audio output error: AudioObjectAddPropertyListener failed, device id 79, prop: [atfp], OSStatus: 1852797029\n",
      "[000000013ef30c50] auhal audio output error: AudioObjectAddPropertyListener failed, device id 50, prop: [atfp], OSStatus: 1852797029\n",
      "[000000011e25cba0] auhal audio output error: AudioObjectAddPropertyListener failed, device id 79, prop: [atfp], OSStatus: 1852797029\n",
      "[000000011e25cba0] auhal audio output error: AudioObjectAddPropertyListener failed, device id 50, prop: [atfp], OSStatus: 1852797029\n"
     ]
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "aa8c3792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bowman, Casady Diane.\\u2009\\n']"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[item+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "c564bd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Van Berkel, Laura.\\u2009\\n'],\n",
       " ['Brown, Nicolas Alexander.\\u2009\\n'],\n",
       " ['Vanden Bosch der Nederlanden, Christina M.\\u2009\\n'],\n",
       " ['Boyd, Tamar.\\u2009\\n'],\n",
       " ['Keum, EunHee.\\u2009\\n'],\n",
       " ['Tafoya, Marsha.\\u2009\\n'],\n",
       " ['Thomalla, Ashley K.\\u2009\\n'],\n",
       " ['Twaite, Jamie T.\\u2009\\n'],\n",
       " ['Babyar, Heather Michelle.\\u2009\\n'],\n",
       " ['Cavanagh, Casey E.\\u2009\\n'],\n",
       " ['Zilkha, Sacha.\\u2009\\n'],\n",
       " ['Lee, Elisa J.\\u2009\\n'],\n",
       " ['Fedder, Joshua Charles.\\u2009\\n'],\n",
       " ['Seror, George A., III.\\u2009\\n'],\n",
       " ['Mustoe, Aaryn.\\u2009\\n'],\n",
       " ['Spano, Goffredina.\\u2009\\n'],\n",
       " ['Freeman, Scott Michael.\\u2009\\n'],\n",
       " ['Chong, Alexandra.\\u2009\\n'],\n",
       " ['Chiang, Jessica.\\u2009\\n'],\n",
       " ['Sherman, Lauren Elizabeth.\\u2009\\n'],\n",
       " ['Jenkins, Keenan M.\\u2009\\n'],\n",
       " ['Picklesimer, Milton E.\\u2009\\n'],\n",
       " ['Susser, Jonathan A.\\u2009\\n'],\n",
       " ['Ford, Cassie Barasch.\\u2009\\n'],\n",
       " ['Hang Smith, Victoria Chialy.\\u2009\\n'],\n",
       " ['Conard, Anna L.\\u2009\\n'],\n",
       " ['Wurpts, Ingrid Carlson.\\u2009\\n'],\n",
       " ['Steding, Lindsey H.\\u2009\\n'],\n",
       " ['Koh, Chee Wee.\\u2009\\n'],\n",
       " ['Knapp, Ashley.\\u2009\\n'],\n",
       " ['Jones, Nickolas Blair.\\u2009\\n'],\n",
       " ['Howorko, Adam.\\u2009\\n'],\n",
       " ['Kanero, Junko.\\u2009\\n'],\n",
       " ['Fettich, Karla C.\\u2009\\n'],\n",
       " ['Shields, Brian J.\\u2009\\n'],\n",
       " ['Hoehn, Jessica L.\\u2009\\n'],\n",
       " ['Travaglini, Letitia E.\\u2009\\n'],\n",
       " ['Rosenthal, A. M.\\u2009\\n'],\n",
       " ['Gamez-Djokic, Monica.\\u2009\\n'],\n",
       " ['Yu, Jing.\\u2009\\n'],\n",
       " ['Thompson, Caitlin C.\\u2009\\n'],\n",
       " ['Scroggins, William Anthony.\\u2009\\n'],\n",
       " ['Kinne, Aubrette.\\u2009\\n'],\n",
       " ['Wolf, Laurie.\\u2009\\n'],\n",
       " ['Carr, Colleen.\\u2009\\n'],\n",
       " ['Cornick, Jessica Elizabeth.\\u2009\\n'],\n",
       " ['Crowell, Adrienne L.\\u2009\\n'],\n",
       " ['Crisafulli, Michele Anne.\\u2009\\n'],\n",
       " ['Wicoff, Maribeth.\\u2009\\n'],\n",
       " ['Plunkett, Lindsay G.\\u2009\\n'],\n",
       " ['Verlenden, Jorge V.\\u2009\\n'],\n",
       " ['Brown, Jeffrey.\\u2009\\n'],\n",
       " ['Van Bommel, Tara.\\u2009\\n'],\n",
       " ['Kapnoula, Efthymia Evangelia.\\u2009\\n'],\n",
       " ['Cooper, Sonia Nowakowska.\\u2009\\n'],\n",
       " ['Steffens, Brent.\\u2009\\n'],\n",
       " ['Lindsey, Dana J.\\u2009\\n'],\n",
       " ['Ortiz-Frontera, Yarimar.\\u2009\\n'],\n",
       " ['Huber, Linda.\\u2009\\n'],\n",
       " ['Harlinger, Mary.\\u2009\\n'],\n",
       " ['Young, Teresa A.\\u2009\\n'],\n",
       " ['Parekh, Mariam.\\u2009\\n'],\n",
       " ['Moravick, Suzanne Marie.\\u2009\\n'],\n",
       " ['Hardy, Sarah.\\u2009\\n'],\n",
       " ['Mills, Mark.\\u2009\\n'],\n",
       " ['Pittenger, Steven T.\\u2009\\n'],\n",
       " ['Fraughton, Tamra Ann.\\u2009\\n'],\n",
       " ['Jones, Winston Edward.\\u2009\\n'],\n",
       " ['Yost, Tyler Andrew.\\u2009\\n'],\n",
       " ['Eisenman, Dwight.\\u2009\\n'],\n",
       " ['Shoemaker, Franklin Edward.\\u2009\\n'],\n",
       " ['Knowles, Odessia.\\u2009\\n'],\n",
       " ['Mackenzie, Eilean L.M.\\u2009\\n'],\n",
       " ['Nabity, Paul S.\\u2009\\n'],\n",
       " ['Clark-McKnight, Gena L.\\u2009\\n'],\n",
       " ['Koehl, Lisa Mason.\\u2009\\n'],\n",
       " ['Roberts, Walter.\\u2009\\n'],\n",
       " [\"O'Handley, Roderick Delphino.\\u2009\\n\"],\n",
       " ['Hackel, Leor M.\\u2009\\n'],\n",
       " ['Stern, Chadly.\\u2009\\n'],\n",
       " ['Nam, Hyun Hannah.\\u2009\\n'],\n",
       " ['Gantman, Ana.\\u2009\\n'],\n",
       " ['Gantman, Ana.\\u2009\\n'],\n",
       " ['Smith, Evan L.\\u2009\\n'],\n",
       " ['Leinenger, Mallorie.\\u2009\\n'],\n",
       " ['Chalik, Lisa.\\u2009\\n'],\n",
       " ['Lempert, Karolina M.\\u2009\\n'],\n",
       " ['Constantino, Sara M.\\u2009\\n'],\n",
       " ['McLemore, Chandler Erin.\\u2009\\n'],\n",
       " ['Ahn, Jina.\\u2009\\n'],\n",
       " ['Thomason, Molly Mishler.\\u2009\\n'],\n",
       " ['Zamzow, Jessica A.\\u2009\\n'],\n",
       " ['Patrick, Kristina Elise.\\u2009\\n'],\n",
       " ['Ward, Deborah Maria.\\u2009\\n'],\n",
       " ['Parker, Kathleen H.\\u2009\\n'],\n",
       " ['Morris, Kasey Lynn.\\u2009\\n'],\n",
       " ['Tuckey, Suzanne.\\u2009\\n'],\n",
       " ['Parker, Thoma J.\\u2009\\n'],\n",
       " ['Pille, Rebecca Owens.\\u2009\\n'],\n",
       " ['Walker, Joyce Lynn.\\u2009\\n']]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "08262e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(map(check, author_uni, values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "893fc62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(map(check, author_uni, values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deecb27c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
